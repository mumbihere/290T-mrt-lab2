{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from typing import Tuple\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import json\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "\n",
    "dataset = pd.read_csv('data/multisession-eeg.csv')\n",
    "fromstring = lambda array_str: np.fromstring(array_str, dtype=float, sep=',')\n",
    "dataset.raw_fft = dataset.raw_fft.apply(fromstring)\n",
    "# dataset.raw_fft.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>subject</th>\n",
       "      <th>session</th>\n",
       "      <th>label</th>\n",
       "      <th>raw_fft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-07-22T20:37:13.267775811Z</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>unlabeled</td>\n",
       "      <td>[10.3113040924, 14.77069664, 12.213514328, 9.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-07-22T20:37:14.253040444Z</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>unlabeled</td>\n",
       "      <td>[11.2151269913, 14.9568557739, 10.8369417191, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-07-22T20:37:15.372317746Z</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>unlabeled</td>\n",
       "      <td>[6.34600162506, 12.5924711227, 10.8416910172, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-07-22T20:37:16.483798739Z</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>unlabeled</td>\n",
       "      <td>[10.0782966614, 16.9934558868, 18.5345039368, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-07-22T20:37:17.471855277Z</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>unlabeled</td>\n",
       "      <td>[4.42960739136, 9.05199050903, 4.41912555695, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             time subject  session      label  \\\n",
       "0  2017-07-22T20:37:13.267775811Z       A        0  unlabeled   \n",
       "1  2017-07-22T20:37:14.253040444Z       A        0  unlabeled   \n",
       "2  2017-07-22T20:37:15.372317746Z       A        0  unlabeled   \n",
       "3  2017-07-22T20:37:16.483798739Z       A        0  unlabeled   \n",
       "4  2017-07-22T20:37:17.471855277Z       A        0  unlabeled   \n",
       "\n",
       "                                             raw_fft  \n",
       "0  [10.3113040924, 14.77069664, 12.213514328, 9.7...  \n",
       "1  [11.2151269913, 14.9568557739, 10.8369417191, ...  \n",
       "2  [6.34600162506, 12.5924711227, 10.8416910172, ...  \n",
       "3  [10.0782966614, 16.9934558868, 18.5345039368, ...  \n",
       "4  [4.42960739136, 9.05199050903, 4.41912555695, ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['A', 'B', 'C'], dtype=object),\n",
       " array(['unlabeled', 'breathe', 'song', 'song_o', 'sport', 'breathe_o',\n",
       "        'speech', 'face', 'calibration', 'word_x', 'phrase_x', 'face_x',\n",
       "        'breatheopen', 'song_x', 'sport_x', 'breatheclosed', 'word_c',\n",
       "        'phrase_c', 'face_c', 'song_c', 'sport_c'], dtype=object))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['subject'].unique(),dataset['label'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passthoughts\n",
    "\n",
    "What if you could simply *think your password*? That's the premise behind *passthoughts*. We'll discuss passthoughts in more depth in lecture 3, but for now, we'll lay this out as a classification problem:\n",
    "\n",
    "> Given a reading, and a person, is that person who they claim to be?\n",
    "\n",
    "We'll structure this problem as follows: For each subject, we'll train a classifier. That subject's readings will be positive example, and everyone else's readings will be negative examples.\n",
    "\n",
    "We can make this a little fancier by having people use specific thoughts (e.g. \"focus on your breathing,\" \"sing a song in your head,\" etc). We'll make sure our methods can handle this case, but for the time being, we'll just use the `\"unabeled\"` readings - people doing nothing in particular.\n",
    "\n",
    "We'll use subject `A` as our \"target\" individual. We will train on this subject for this assignment, and train against the other subjects in the corpus (subjects `B` and `C`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_matrix (series):\n",
    "    return np.array([ x for x in series ])\n",
    "\n",
    "def readings_right_subject_right_task (subj, task, session=0):\n",
    "    return to_matrix(dataset[\n",
    "        (dataset['subject'] == subj) &\n",
    "        (dataset['label'] == task) &\n",
    "        (dataset['session'] == session)\n",
    "    ].raw_fft)\n",
    "\n",
    "def readings_wrong_subj_any_task (subj):\n",
    "    return to_matrix(dataset[\n",
    "        (dataset['subject'] != subj)\n",
    "    ].raw_fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40, 516), (1228, 516))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive = readings_right_subject_right_task('A', 'unlabeled', 0)\n",
    "negative = readings_wrong_subj_any_task('A')\n",
    "positive.shape, negative.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "Notice how we structured our positive and negative examples:\n",
    "\n",
    "- *Positive examples*: The right person thinking the right task.\n",
    "\n",
    "- *Negative examples*: The wrong person thinking any task (whether it is right or wrong).\n",
    "\n",
    "In the context of passthoughts, consider other possibilites for selecting positive and negative features. Here, (1) pick one configuration of positive and negative examples, aside from the ones listed, and (2) discuss their possible consequences (pros/cons). Explain how you might evaluate this selection (with data, with user experiments, etc - your choice)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Your answer here...*\n",
    "\n",
    "Another possible configuration of positive and negative examples is:\n",
    "- *Positive example*:*: The right person thinking any task\n",
    "- *Negative examples*: The wrong person thinking any task (whether it is right or wrong) \n",
    "\n",
    "This configuration puts the emphasis on the subject rather than the password. It assumes that it is more important to authenticate the right person(regardless of what passthought they have) than to always have a correct password/passthought presented.In a way, this configuration undermines the knowledge factor while primarily focussing on the inherence factor.\n",
    " \n",
    "##### Explain how you might evaluate this selection (with data, with user experiments, etc - your choice)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll turn these data into our feature/label matrices `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.concatenate([positive, negative])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = np.array([ 0 for x in positive] + [ 1 for x in negative])\n",
    "assert X.shape[0] == y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are assigning `0` to \"positive\" examples, and `1` to \"negative\" examples. That means `0` will mean \"ACCEPT\" and `1` will mean \"REJECT.\"\n",
    "\n",
    "## TODO\n",
    "\n",
    "Now, train and test a classifier! Estimate your classifier's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here....\n",
    "\n",
    "def fresh_clf () -> XGBClassifier:\n",
    "    return XGBClassifier(\n",
    "        # Don't worry about those parameters for now,\n",
    "        # though feel free to look them up if you're interested.\n",
    "        objective= 'binary:logistic',\n",
    "        seed=27)\n",
    "\n",
    "def xgb_cross_validate (\n",
    "    X: np.array,\n",
    "    y: np.array,\n",
    "    nfold: int=7\n",
    ") -> Tuple[XGBClassifier, pd.DataFrame]:\n",
    "    # eval_metrics:\n",
    "    # http://xgboost.readthedocs.io/en/latest//parameter.html\n",
    "    metrics = ['error@0.1', 'auc']\n",
    "#     metrics = [ 'auc' ]\n",
    "    # we use the @ syntax to override the default of 0.5 as the threshold for 0 / 1 classification\n",
    "    # the intent here to to minimize FAR at the expense of FRR\n",
    "    alg = fresh_clf()\n",
    "    xgtrain = xgb.DMatrix(X,y)\n",
    "    param = alg.get_xgb_params()\n",
    "    cvresults = xgb.cv(param,\n",
    "                      xgtrain,\n",
    "                      num_boost_round=alg.get_params()['n_estimators'],\n",
    "                      nfold=nfold,\n",
    "                      metrics=metrics,\n",
    "                      early_stopping_rounds=100\n",
    "                      )\n",
    "    alg.set_params(n_estimators=cvresults.shape[0])\n",
    "    alg.fit(X,y,eval_metric=metrics)\n",
    "    return alg, cvresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_validate, y_train, y_validate = model_selection.train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.33, \n",
    "    random_state=42)\n",
    "\n",
    "clf, cvres = xgb_cross_validate(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98329355608591884"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_validate, y_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For authentication, what we want even more than \"accuracy\" here are two metrics:\n",
    "\n",
    "- False Acceptance Rate (FAR): The percentage of readings *not* from subject A incorrectly classified \"ACCEPT.\"\n",
    "- False Rejection Rate (FRR): The percentage of readings *from* subject A incorrectly classified 'REJECT.\"\n",
    "\n",
    "For authentication /security/, we want FAR to be as low as possible (so nobody can break in).\n",
    "For authentication /usability/, we want FRR to be low (so user's don't get frustrated constantly re-trying their passthought)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def far_frr (classifier, features, labels):\n",
    "    # predict all the labels\n",
    "    y_pred = classifier.predict(features)\n",
    "    false_accepts = 0\n",
    "    false_rejects = 0\n",
    "    for predicted, actual in zip(y_pred, labels):\n",
    "        # if we should have rejected,\n",
    "        # but in fact accepted,\n",
    "        if (actual == 1) and (predicted == 0):\n",
    "            # increment false accepts\n",
    "            false_accepts += 1\n",
    "        # if we should have accepted,\n",
    "        # but in fact rejected,\n",
    "        if (actual == 0) and (predicted == 1):\n",
    "            # increment false rejections\n",
    "            false_rejects += 1\n",
    "    # calculate proportions for each\n",
    "    far = false_accepts / len(list(filter(lambda x: x ==0,y_pred)))\n",
    "    frr = false_rejects / len(list(filter(lambda x: x ==1,y_pred)))\n",
    "    return far, frr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3, 0.009779951100244499)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "far, frr = far_frr(clf, X_validate, y_validate)\n",
    "#'FAR: {far} - FRR: {frr}'\n",
    "far,frr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, these results might be good. \n",
    "\n",
    "But our classifier's accuracy could be misleading.   \n",
    "\n",
    "Can you see why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two reasons why our classifier might be misleading:\n",
    "- The number of training examples for the positive-negative classes are 40-1228 (1:~30) in the example above. \n",
    "\n",
    "- This classifier is not explicitly trained on readings of the right person thinking the wrong thing. These readings can easily be treated as positive or negative depending on viewpoint:\n",
    "\n",
    "   An approach that assumes that the knowledge factor is more important(knowledge-centric approach), might assert that the right person with the wrong password is always rejected. Alternatively, an inherence-centric viewpoint would treat such examples as positive examples where the right person is always accepted. This ambiguity could perhaps explain the FAR to a small degree.\n",
    "   \n",
    "I test this hypothesis below:\n",
    "###### 1. inherence-centric viewpoint\n",
    "Here, I assume the configuration:\n",
    "- Positive example:*: The right person thinking any task.\n",
    "- Negative examples: The wrong person thinking any task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive examples:  (1552, 516)  negative examples:  (1228, 516)\n",
      "\n",
      "Our classifier has an accuracy of:  0.99128540305\n",
      "\n",
      "FAR: 0.011627906976744186  FRR:  0.004975124378109453\n"
     ]
    }
   ],
   "source": [
    "#generic function for training and checking frr given positive and negative examples\n",
    "def train_check_far_frr(positive,negative):\n",
    "    #Check dimensions\n",
    "    positive.shape, negative.shape\n",
    "    print('positive examples: ', positive.shape,' negative examples: ', negative.shape)\n",
    "\n",
    "    #Train a classifier\n",
    "    X = np.concatenate([positive, negative])\n",
    "    y = np.array([ 0 for x in positive] + [ 1 for x in negative])\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    X_train, X_validate, y_train, y_validate = model_selection.train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    clf, cvres = xgb_cross_validate(X_train, y_train)\n",
    "    print('\\nOur classifier has an accuracy of: ',clf.score(X_validate, y_validate))\n",
    "\n",
    "    #Check FAR and FRR\n",
    "    far, frr = far_frr(clf, X_validate, y_validate)\n",
    "    print('\\nFAR:', far,' FRR: ',frr)\n",
    "    \n",
    "def readings_right_subj (subj):\n",
    "    return to_matrix(dataset[\n",
    "        (dataset['subject'] == subj)\n",
    "    ].raw_fft)\n",
    "\n",
    "\n",
    "positive = readings_right_subj('A')\n",
    "negative = readings_wrong_subj_any_task('A')\n",
    "\n",
    "train_check_far_frr(positive,negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, I assume the configuration:\n",
    "- *Positive example*:*: The right person thinking any task. \n",
    "- *Negative examples*: The wrong person thinking any task \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. knowledge-centric viewpoint\n",
    "Here, I assume the configuration:\n",
    "- Positive example:*: The right person thinking the right task.\n",
    "- Negative examples: The wrong person thinking any task + right person thinking the wrong task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive examples:  (40, 516)  negative examples:  (1410, 516)\n",
      "\n",
      "Our classifier has an accuracy of:  0.983298538622\n",
      "\n",
      "FAR: 1.0  FRR:  0.014644351464435146\n"
     ]
    }
   ],
   "source": [
    "def readings_right_subject_wrong_task (subj, task, session=0):\n",
    "    return to_matrix(dataset[\n",
    "        (dataset['subject'] == subj) &\n",
    "        (dataset['label'] != task) &\n",
    "        (dataset['session'] == session)\n",
    "    ].raw_fft)\n",
    "\n",
    "positive = readings_right_subject_right_task('A', 'unlabeled', 0)\n",
    "negative = np.concatenate([readings_wrong_subj_any_task('A'),readings_right_subject_wrong_task('A', 'unlabeled')])\n",
    "\n",
    "train_check_far_frr(positive,negative)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach performs way worse than all other configurations seen so far. Shockingly, the FAR is at 100% (everyone who was accepted was a fraud!!). This could potentially be attributed to the overwhelming number of negative examples. To handle this, I reduce the number of negative examples by shuffling and slicing the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive examples:  (40, 516)  negative examples:  (50, 516)\n",
      "\n",
      "Our classifier has an accuracy of:  0.8\n",
      "\n",
      "FAR: 0.1  FRR:  0.25\n"
     ]
    }
   ],
   "source": [
    "#Shuffle and slice the negative examples so that the training set is not too unbalanced.\n",
    "np.random.shuffle(negative)\n",
    "neg = negative[0:50,:]\n",
    "train_check_far_frr(positive,neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the FAR reduces the classifier's accuracy drops significantly. Overall this could mean that the knowledge factor is not as important in authenticating users as the inherence factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonstationarity\n",
    "\n",
    "We are training, and testing, using data recorded over a single session. As we know, EEG changes over time, a property known as *nonstationarity*. Will our great results still hold a few weeks later?\n",
    "\n",
    "Let's take subject `A`'s data from sessions 1 and 2, which were recorded a few weeks after session 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_subja_sess1 = readings_right_subject_right_task('A', 'unlabeled', 1)\n",
    "X_subja_sess2 = readings_right_subject_right_task('A', 'unlabeled', 2)\n",
    "X_subja_later = np.concatenate([X_subja_sess1, X_subja_sess2])\n",
    "y_subja_later = [ 0 for x in X_subja_later ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try the classifier we trained on the original data, testing it on the later data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "far, frr = far_frr(clf, X_subja_later, y_subja_later)\n",
    "#f'FAR: {far*100}% - FRR: {frr*100}%'\n",
    "far,frr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will discuss more in lecture 3, this is a problem for us. After all, we can calibrate our target subject, but we then expect them to leave the lab and go use the device later on. If their state changes so much that they can no longer be authenticated, we can't very well claim our system is accurate!\n",
    "\n",
    "## TODO\n",
    "\n",
    "The crux of the lab focuses on nonstationarity. At minimum, your mission is to quantify and qualify *what* is changing in EEG signals over time. You may use any tools in answering this question.\n",
    "\n",
    "You also have your choice of corpus:\n",
    "\n",
    "- Study subject `A`'s recordings over the three sessions provided here.\n",
    "- Study one subject's recordings over the course of a year.\n",
    "\n",
    "You can use both of these corpora, if you would like.\n",
    "\n",
    "Some questions to spur investigation:\n",
    "\n",
    "- What features of readings cause a classifier that works on earlier recordings fail on later ones?\n",
    "- What features remain the same? Are there any?\n",
    "- What might be the source of these changing features? Changing placement in the EEG device? Changing properties of the brain?\n",
    "\n",
    "Please note below all work you do, and any notes you make along the way. Ideally, your work should read like a story - words (and questions!) interspersed with code. Good luck, and have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(656, 516)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prepare the data\n",
    "\n",
    "X_subja_sess1 = readings_right_subject_right_task('A', 'unlabeled', 0)\n",
    "X_subja_sess2 = readings_right_subject_right_task('A', 'unlabeled', 1)\n",
    "X_subja_sess3 = readings_right_subject_right_task('A', 'unlabeled', 2)\n",
    "\n",
    "y_subja_sess1 = [ 0 for x in X_subja_sess1 ]\n",
    "y_subja_sess2 = [ 1 for x in X_subja_sess2 ]\n",
    "y_subja_sess3 = [ 2 for x in X_subja_sess3 ]\n",
    "\n",
    "X = np.concatenate([X_subja_sess1, X_subja_sess2,X_subja_sess3])\n",
    "\n",
    "\n",
    "y = np.concatenate([y_subja_sess1,y_subja_sess2,y_subja_sess3])\n",
    "target_names = ['session 1','session 2','session 3']\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "n_components=3\n",
    "\n",
    "def plot_PCA(X,y,target_names,n_components):\n",
    "    pca = PCA(n_components)\n",
    "    X_r = pca.fit(X).transform(X)\n",
    "    # Percentage of variance explained for each components\n",
    "#     print('explained variance ratio (first two components): %s'\n",
    "#           % str(pca.explained_variance_ratio_))\n",
    "\n",
    "#     plt.figure()\n",
    "#     colors = ['navy', 'turquoise', 'darkorange']\n",
    "#     lw = 2\n",
    "#     for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
    "#         plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=.8, lw=lw,\n",
    "#                     label=target_name)\n",
    "#     plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "#     plt.title('PCA Viz')\n",
    "#     plt.figure()\n",
    "\n",
    "def plot_LDA(X,y):\n",
    "    lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "    X_r2 = lda.fit(X, y).transform(X)\n",
    "    for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
    "        plt.scatter(X_r2[y == i, 0], X_r2[y == i, 1], alpha=.8, color=color,\n",
    "                    label=target_name)\n",
    "    plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "    plt.title('LDA Viz')\n",
    "\n",
    "plot_PCA(X,y,target_names,n_components)\n",
    "#plot_LDA(X,y)\n",
    "\n",
    "print(target_names)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
